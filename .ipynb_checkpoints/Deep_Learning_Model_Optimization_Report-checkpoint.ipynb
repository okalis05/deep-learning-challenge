{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f148d7fd-8e48-49a1-8568-ac14130ab402",
   "metadata": {},
   "source": [
    "# Credit Risk Classification report\n",
    " ---\n",
    "\n",
    "## Overview of the Analysis\n",
    "---\n",
    "The nonprofit foundation `Alphabet Soup` wants a tool that can help it select the applicants for funding with the best chance of success in  their ventures and we were tasked to use the features in the provided dataset to create a binary classifier that can predict whether      applicants will be successful if funded by Alphabet Soup.\n",
    "\n",
    "---\n",
    "### Purpose of the analysis\n",
    "---\n",
    "For this analysis, we were given a dataset composed of 8 columns and 77536 rows holding information about credit borrowers and we were tasked with predicting their credit worthiness or the outcome of each loan application\n",
    "\n",
    "### Methodology\n",
    "---\n",
    "#### 1-Preprocessing\n",
    "\n",
    "- First we imported the necessary dependencies , read and stored the `charity_data.csv ` file into a pandas dataframe named `application_df`.\n",
    "- After dropping unwanted columns 'EIN' and 'NAME' and determining the count of unique values in each remaining column , we identified less recurring values in columns'APPLICATION_TYPE' and 'CLASSIFICATION' which we binned in the category named'Other'.\n",
    "- Then we converted categorical data to numeric with `pd.get_dummies`\n",
    "- Furthermore we splitted our preprocessed data into our features and target arrays then into training and testing dataset\n",
    "- Finally we created a `StandardScaler` instance ,then fitted it and scaled the data\n",
    "\n",
    "#### 2-Compile, Train and Evaluate the Model\n",
    "- In this portion , we defined a `deep learning` model with 2 hidden layers using the `relu` activation function and an ouput layer using the `sigmoid` activation function.\n",
    "- Next , we compiled ,trained using the training data at 'epochs=100' then evaluated the model using the 'test data'.\n",
    "- furthermore, we repeated the process 3 more times , using various aoptimization method to get the highest accuracy score\n",
    "* 1- first we increased the number of hidden layers from 2 to 3 , giving each 10 nodes respectively.We also assigned the `relu` activation function to the input layer ,the `tanh` to the second and third function and the `sigmoid` function to the output layer.Finally we compiled the model with epochs=300.\n",
    "* 2- Secondly ,repeating the preprocessing process and keeping the number of hidden layers(3) and changing the nodes to 20 for each layers , we compiled the model with epochs = 1000.\n",
    "* 3-\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Results\n",
    "\n",
    "Our dataset had 77536 rows.\n",
    "\n",
    "The confusion matrix returned an array of 19384 values ,representing about a quater of our dataset used to test the data.\n",
    "On the confusion matrix , we are looking at how many times the model predicted 0 where 0 was due , 1 where 1 was due and also the number of times it confused the 0 for 1 and vice versa.We can see that it predicted 'healthy loan' correctly 18663 correctly and 'high risk loan' correctly 563 times out of the 19384 times it ran through the testing dataset, leaving a margin of error of just 158 accross both components.This means that the accuracy of the model prediction sit at a whooping 99%.\n",
    "For the Healthy Loans (0), the precision is 1.00, indicating that all the instances for this label were correctly identified . The recall is 0.99, meaning that 99% of the instances of this label were correctly classified.\n",
    "For the High-Risk Loan (1) the precision is 0.85, indicating that 85% of instances for this label were accurately identified. The recall is 0.91, indicating that 91% of the instances of this label were correctly classified.\n",
    "This is a very high score for our model ,but a closer look at the data distribution accross both components shows that out of the 19384 values in our test data , 18765 represented the total ofhealthy loan applications versus '619' representing the high risk loan application .This, in addition to the lower precision score on the high-risk loanmeans that we trained our model to recognised mostly healthy loan which signals a bias in our analysis ,because even if the model somehow marked 0 all the way down, it would have still scored a whooping 97% which represents the percentage of 'healthy loan' fed to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43962a-5d75-452f-9347-c7e5caae14bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
